Human Motion Retargeting from RGB Images

This repository presents an end-to-end pipeline for human motion retargeting, starting from raw RGB images and producing a fully animated, identity-preserving 3D human avatar. The system integrates multi-view 3D reconstruction, parametric human modeling with SMPL-X, appearance transfer, and motion retargeting using real motion data.

The project was developed as part of STAT 59800: 3D Computer Vision & Virtual Human Models.

â¸»

âœ¨ Highlights
	â€¢	ğŸ“¸ Image-based 3D reconstruction using COLMAP + PyCOLMAP (CUDA)
	â€¢	ğŸ§ Parametric human fitting with SMPL-X via multi-stage optimization
	â€¢	ğŸ¨ Robust color transfer from reconstructed scans to SMPL-X
	â€¢	ğŸ§¹ Point cloud & mesh refinement (SOR, noise filtering, Poisson reconstruction)
	â€¢	ğŸƒ Motion retargeting using MOYO motion sequences
	â€¢	ğŸ¬ Final animated video output with preserved identity and appearance

â¸»

ğŸ§  Pipeline Overview

RGB Images
   â†“
Foreground Masking (YOLO + SAM)
   â†“
Multi-view Reconstruction (COLMAP / PyCOLMAP)
   â†“
Point Cloud Refinement (SOR + Noise Filtering)
   â†“
Poisson Surface Reconstruction
   â†“
Mesh Refinement & Triangle Decimation
   â†“
SMPL-X Fitting (Multi-stage Optimization)
   â†“
Color Transfer (Nearest Neighbor Projection)
   â†“
Motion Retargeting (MOYO)
   â†“
Final Animated Video


â¸»

ğŸ“ Repository Structure

.
â”œâ”€â”€ reconstruction/
â”‚   â”œâ”€â”€ colmap_pipeline.py
â”‚   â””â”€â”€ point_cloud_refinement.py
â”‚
â”œâ”€â”€ mesh_processing/
â”‚   â”œâ”€â”€ poisson_reconstruction.py
â”‚   â”œâ”€â”€ mesh_refinement.py
â”‚   â””â”€â”€ mesh_decimation.py
â”‚
â”œâ”€â”€ smplx_fitting/
â”‚   â”œâ”€â”€ fit_smplx.py
â”‚   â”œâ”€â”€ loss_plot.py
â”‚   â””â”€â”€ debug_visualization.py
â”‚
â”œâ”€â”€ appearance_transfer/
â”‚   â””â”€â”€ color_transfer.py
â”‚
â”œâ”€â”€ motion_retargeting/
â”‚   â””â”€â”€ retarget_moyo_motion.py
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ videos/
â”‚   â””â”€â”€ figures/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


â¸»

ğŸ”§ Installation

1. Environment Setup

conda create -n human_motion python=3.9
conda activate human_motion

2. Install Dependencies

pip install -r requirements.txt

Key dependencies include:
	â€¢	torch
	â€¢	smplx
	â€¢	trimesh
	â€¢	pycolmap
	â€¢	open3d
	â€¢	scikit-learn

Note: PyCOLMAP with CUDA support is required for dense multi-view stereo.

â¸»

ğŸ“¸ Foreground Masking

Foreground human segmentation is achieved via a two-stage learning-based pipeline:
	â€¢	YOLO for coarse human detection (bounding boxes)
	â€¢	SAM (Segment Anything Model) for precise pixel-level masks

This step removes background clutter and significantly improves reconstruction quality.

â¸»

â˜ï¸ 3D Reconstruction
	â€¢	Multi-view images are reconstructed using COLMAP / PyCOLMAP (CUDA)
	â€¢	Outputs include camera poses, sparse points, and dense point clouds
	â€¢	Typical scale:
	â€¢	Initial points: ~8.3M
	â€¢	Refined points: ~1.2M

â¸»

ğŸ§¹ Point Cloud & Mesh Processing

Point Cloud Refinement
	â€¢	Statistical Outlier Removal (SOR)
	â€¢	Noise filtering (CloudCompare-style)

Mesh Construction
	â€¢	Poisson Surface Reconstruction (depth â‰ˆ 9)
	â€¢	Produces watertight, manifold meshes

Mesh Optimization
	â€¢	Laplacian smoothing
	â€¢	Removal of disconnected components
	â€¢	Triangle decimation (Quadric Edge Collapse)
	â€¢	~840K â†’ ~200K faces

â¸»

ğŸ§ SMPL-X Fitting

We fit an SMPL-X model to the refined mesh using multi-stage gradient-based optimization.

Optimization Stages

Stage	Parameters	Learning Rate	Iterations
1	Global orient, translation, scale	0.01	150
2	Shape (Î²)	0.01	250
3	Pose (high reg)	0.005	300
4	Pose refinement	0.002	400
5	All parameters	0.001	200

	â€¢	Loss: Bidirectional Chamfer distance
	â€¢	Regularization: Shape, pose, and hand priors

â¸»

ğŸ¨ Appearance Transfer

Color is transferred from the reconstructed scan to the SMPL-X mesh via nearest-neighbor projection:
	â€¢	Scan and SMPL-X meshes are independently centered
	â€¢	Vertex colors are normalized to [0, 1]
	â€¢	k-NN projection (k = 1) in 3D space

This avoids UV mapping and remains robust to alignment offsets.

â¸»

ğŸƒ Motion Retargeting (MOYO)

Motion is retargeted using sequences from the MOYO dataset:
	â€¢	Pose representation: 165D fullpose
	â€¢	Body, hand, and jaw poses extracted explicitly
	â€¢	Shape (Î²) and scale fixed across all frames
	â€¢	Floor alignment via minimum vertex height
	â€¢	Frame-wise export as OBJ

â¸»

ğŸ¬ Final Output
	â€¢	Colored SMPL-X avatar animated with retargeted motion
	â€¢	Rendered as a video (MP4) from exported frame sequence

<p align="center">
  <img src="assets/videos/final_animation.gif" width="600" />
</p>



â¸»

âš ï¸ Limitations
	â€¢	Fine hand and finger geometry may be incomplete due to reconstruction sparsity
	â€¢	Facial expressions are limited by scan quality
	â€¢	No temporal smoothing across motion frames

â¸»

ğŸš€ Future Work
	â€¢	Temporal consistency constraints during fitting
	â€¢	Improved hand reconstruction
	â€¢	Texture atlas generation
	â€¢	Real-time rendering via Vertex Animation Textures (VAT)

â¸»

ğŸ“š Acknowledgements
	â€¢	SMPL-X: AMASS / MPI
	â€¢	COLMAP / PyCOLMAP
	â€¢	MOYO Dataset
	â€¢	Segment Anything (SAM)

â¸»

ğŸ‘¤ Author

Vishal Purohit
PhD Student â€” 3D Vision & Generative Models

â¸»

ğŸ“¬ Contact

For questions or collaborations, feel free to reach out.